######################################### STEP 0 #########################################
< action >
[ 0.00031904 -0.00024937]
< returned_observation >
[ 0.41734104  0.72007512]
< reward >
2.6000000000000014
< running_reward >
0.7800000000000004
< done >
False
######################################### STEP 1 #########################################
< action >
[ 0.00178061 -0.00230909]
< returned_observation >
[ 0.41912165  0.71776604]
< reward >
1.1000000000000014
< running_reward >
0.3300000000000004
< done >
False
######################################### STEP 2 #########################################
< action >
[ 0.00145516 -0.00268922]
< returned_observation >
[ 0.42057681  0.71507682]
< reward >
-12.399999999999999
< running_reward >
-3.7199999999999993
< done >
False
######################################### STEP 3 #########################################
< action >
[ 0.00258645 -0.00378453]
< returned_observation >
[ 0.42316326  0.71129229]
< reward >
-11.8
< running_reward >
-3.54
< done >
False
######################################### STEP 4 #########################################
< action >
[ 0.00240963 -0.00465596]
< returned_observation >
[ 0.42557289  0.70663633]
< reward >
-13.100000000000001
< running_reward >
-3.93
< done >
False
######################################### STEP 5 #########################################
< action >
[ 0.00244775 -0.00406523]
< returned_observation >
[ 0.42802064  0.7025711 ]
< reward >
2.3000000000000007
< running_reward >
0.6900000000000002
< done >
False
######################################### STEP 6 #########################################
< action >
[ 0.00134297 -0.00291359]
< returned_observation >
[ 0.42936361  0.6996575 ]
< reward >
1.0
< running_reward >
0.3
< done >
False
######################################### STEP 7 #########################################
< action >
[ 0.00224227 -0.00240615]
< returned_observation >
[ 0.43160588  0.69725135]
< reward >
0.8000000000000007
< running_reward >
0.2400000000000002
< done >
False
######################################### STEP 8 #########################################
< action >
[ 0.00313932 -0.00308579]
< returned_observation >
[ 0.4347452   0.69416557]
< reward >
2.3999999999999986
< running_reward >
0.7199999999999995
< done >
False
######################################### STEP 9 #########################################
< action >
[ 0.0030111  -0.00401631]
< returned_observation >
[ 0.4377563   0.69014926]
< reward >
-12.0
< running_reward >
-3.5999999999999996
< done >
False
######################################### STEP 10 #########################################
< action >
[ 0.00273809 -0.00347913]
< returned_observation >
[ 0.44049439  0.68667013]
< reward >
-11.899999999999999
< running_reward >
-3.5699999999999994
< done >
False
######################################### STEP 11 #########################################
< action >
[ 0.00204177 -0.00386997]
< returned_observation >
[ 0.44253615  0.68280017]
< reward >
3.6000000000000014
< running_reward >
1.0800000000000003
< done >
False
######################################### STEP 12 #########################################
< action >
[ 0.00135112 -0.00470859]
< returned_observation >
[ 0.44388728  0.67809157]
< reward >
-10.8
< running_reward >
-3.24
< done >
False
######################################### STEP 13 #########################################
< action >
[ 0.00067759 -0.00471325]
< returned_observation >
[ 0.44456486  0.67337832]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 14 #########################################
< action >
[-0.00044088 -0.00447082]
< returned_observation >
[ 0.44412399  0.6689075 ]
< reward >
2.6999999999999993
< running_reward >
0.8099999999999997
< done >
False
######################################### STEP 15 #########################################
< action >
[ 0.00121967 -0.00372118]
< returned_observation >
[ 0.44534366  0.66518632]
< reward >
1.8000000000000007
< running_reward >
0.5400000000000001
< done >
False
######################################### STEP 16 #########################################
< action >
[ 0.00102577 -0.00460248]
< returned_observation >
[ 0.44636942  0.66058384]
< reward >
2.0
< running_reward >
0.6
< done >
False
######################################### STEP 17 #########################################
< action >
[ 0.00027686 -0.0029022 ]
< returned_observation >
[ 0.44664629  0.65768164]
< reward >
3.0
< running_reward >
0.8999999999999999
< done >
False
######################################### STEP 18 #########################################
< action >
[ 0.0003272  -0.00353426]
< returned_observation >
[ 0.44697349  0.65414737]
< reward >
1.6999999999999993
< running_reward >
0.5099999999999998
< done >
False
######################################### STEP 19 #########################################
< action >
[ 0.00051756 -0.001428  ]
< returned_observation >
[ 0.44749105  0.65271937]
< reward >
-13.100000000000001
< running_reward >
-3.93
< done >
False
######################################### STEP 20 #########################################
< action >
[ 0.00063684 -0.00080837]
< returned_observation >
[ 0.44812789  0.651911  ]
< reward >
1.1999999999999993
< running_reward >
0.35999999999999976
< done >
False
######################################### STEP 21 #########################################
< action >
[ 0.00093592 -0.00115925]
< returned_observation >
[ 0.44906381  0.65075175]
< reward >
-12.399999999999999
< running_reward >
-3.7199999999999993
< done >
False
######################################### STEP 22 #########################################
< action >
[-0.00020818 -0.00150662]
< returned_observation >
[ 0.44885563  0.64924513]
< reward >
0.3000000000000007
< running_reward >
0.0900000000000002
< done >
False
######################################### STEP 23 #########################################
< action >
[-0.00041673 -0.00091743]
< returned_observation >
[ 0.4484389  0.6483277]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 24 #########################################
< action >
[  4.22966480e-04   1.52289867e-05]
< returned_observation >
[ 0.44886187  0.64834293]
< reward >
3.6000000000000014
< running_reward >
1.0800000000000003
< done >
False
######################################### STEP 25 #########################################
< action >
[ 0.00070783  0.00090035]
< returned_observation >
[ 0.4495697   0.64924328]
< reward >
-12.100000000000001
< running_reward >
-3.6300000000000003
< done >
False
######################################### STEP 26 #########################################
< action >
[ -4.77671623e-05   2.15167999e-03]
< returned_observation >
[ 0.44952193  0.65139496]
< reward >
2.3000000000000007
< running_reward >
0.6900000000000002
< done >
False
######################################### STEP 27 #########################################
< action >
[ 0.00046524  0.00184993]
< returned_observation >
[ 0.44998718  0.65324489]
< reward >
-11.899999999999999
< running_reward >
-3.5699999999999994
< done >
False
######################################### STEP 28 #########################################
< action >
[ 0.00095297  0.00177121]
< returned_observation >
[ 0.45094015  0.6550161 ]
< reward >
0.8000000000000007
< running_reward >
0.2400000000000002
< done >
False
######################################### STEP 29 #########################################
< action >
[ 0.00208298  0.00328802]
< returned_observation >
[ 0.45302313  0.65830412]
< reward >
2.0
< running_reward >
0.6
< done >
True
##################### episode 0 infomation #####################
{'episode_reward': -108.79999999999997, 'ave_reward': -3.6266666666666656, 'max_reward': 3.6000000000000014, 'min_reward': -13.100000000000001, 'nb_episode_steps': 30, 'nb_steps': 30}
######################################### STEP 30 #########################################
< action >
[-0.00144411 -0.00050447]
< returned_observation >
[ 0.          0.30182811]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 31 #########################################
< action >
[-0.00128162  0.00037256]
< returned_observation >
[ 0.          0.30220066]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 32 #########################################
< action >
[-0.00096381 -0.00165027]
< returned_observation >
[ 0.          0.30055039]
< reward >
-12.7
< running_reward >
-3.8099999999999996
< done >
False
######################################### STEP 33 #########################################
< action >
[-0.00126837 -0.00081949]
< returned_observation >
[ 0.         0.2997309]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 34 #########################################
< action >
[ -1.03612244e-03  -5.60909510e-05]
< returned_observation >
[ 0.          0.29967481]
< reward >
3.5
< running_reward >
1.05
< done >
False
######################################### STEP 35 #########################################
< action >
[-0.00125669 -0.00025675]
< returned_observation >
[ 0.          0.29941805]
< reward >
-12.3
< running_reward >
-3.69
< done >
False
######################################### STEP 36 #########################################
< action >
[-0.00106799  0.00015374]
< returned_observation >
[ 0.          0.29957179]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 37 #########################################
< action >
[-0.00086788  0.00027248]
< returned_observation >
[ 0.          0.29984428]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 38 #########################################
< action >
[-0.00153706  0.00064958]
< returned_observation >
[ 0.          0.30049386]
< reward >
2.8999999999999986
< running_reward >
0.8699999999999996
< done >
False
######################################### STEP 39 #########################################
< action >
[-0.00141263  0.00177796]
< returned_observation >
[ 0.          0.30227182]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 40 #########################################
< action >
[-0.00021131  0.0019601 ]
< returned_observation >
[ 0.          0.30423192]
< reward >
2.1999999999999993
< running_reward >
0.6599999999999998
< done >
False
######################################### STEP 41 #########################################
< action >
[-0.00058624  0.00131804]
< returned_observation >
[ 0.          0.30554996]
< reward >
-11.600000000000001
< running_reward >
-3.4800000000000004
< done >
False
######################################### STEP 42 #########################################
< action >
[-0.00016174  0.00139313]
< returned_observation >
[ 0.          0.30694309]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 43 #########################################
< action >
[-0.00050532  0.00143436]
< returned_observation >
[ 0.          0.30837746]
< reward >
-12.100000000000001
< running_reward >
-3.6300000000000003
< done >
False
######################################### STEP 44 #########################################
< action >
[-0.00112447  0.00212995]
< returned_observation >
[ 0.          0.31050741]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 45 #########################################
< action >
[-0.00156968  0.00335084]
< returned_observation >
[ 0.          0.31385825]
< reward >
2.3999999999999986
< running_reward >
0.7199999999999995
< done >
False
######################################### STEP 46 #########################################
< action >
[-0.00116352  0.00393872]
< returned_observation >
[ 0.          0.31779697]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 47 #########################################
< action >
[-0.00225646  0.00410141]
< returned_observation >
[ 0.          0.32189839]
< reward >
2.1000000000000014
< running_reward >
0.6300000000000004
< done >
False
######################################### STEP 48 #########################################
< action >
[-0.00151206  0.00314074]
< returned_observation >
[ 0.          0.32503913]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 49 #########################################
< action >
[-0.00177571  0.00316802]
< returned_observation >
[ 0.          0.32820715]
< reward >
-11.0
< running_reward >
-3.3
< done >
False
######################################### STEP 50 #########################################
< action >
[-0.00314581  0.00347779]
< returned_observation >
[ 0.          0.33168493]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 51 #########################################
< action >
[-0.0022943   0.00261236]
< returned_observation >
[ 0.          0.33429729]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 52 #########################################
< action >
[-0.00193985  0.00129564]
< returned_observation >
[ 0.          0.33559293]
< reward >
3.0
< running_reward >
0.8999999999999999
< done >
False
######################################### STEP 53 #########################################
< action >
[-0.00197525 -0.00032234]
< returned_observation >
[ 0.          0.33527059]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 54 #########################################
< action >
[ -8.50477815e-04   8.71062279e-05]
< returned_observation >
[ 0.         0.3353577]
< reward >
2.3999999999999986
< running_reward >
0.7199999999999995
< done >
False
######################################### STEP 55 #########################################
< action >
[-0.00087365 -0.0006882 ]
< returned_observation >
[ 0.         0.3346695]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 56 #########################################
< action >
[ 0.00040159  0.00128007]
< returned_observation >
[ 0.00040159  0.33594957]
< reward >
-11.0
< running_reward >
-3.3
< done >
False
######################################### STEP 57 #########################################
< action >
[-0.00145707  0.00251406]
< returned_observation >
[ 0.          0.33846362]
< reward >
-11.0
< running_reward >
-3.3
< done >
False
######################################### STEP 58 #########################################
< action >
[ 0.00017306  0.0028478 ]
< returned_observation >
[  1.73056126e-04   3.41311420e-01]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 59 #########################################
< action >
[-0.00102651  0.0037063 ]
< returned_observation >
[ 0.          0.34501772]
< reward >
-11.8
< running_reward >
-3.54
< done >
True
##################### episode 1 infomation #####################
{'episode_reward': -244.89999999999995, 'ave_reward': -8.163333333333332, 'max_reward': 3.5, 'min_reward': -12.7, 'nb_episode_steps': 30, 'nb_steps': 60}
######################################### STEP 60 #########################################
< action >
[-0.00123006  0.00055054]
< returned_observation >
[ 0.14552583  0.09288913]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 61 #########################################
< action >
[ -4.35161591e-04  -7.39306211e-05]
< returned_observation >
[ 0.14509067  0.0928152 ]
< reward >
-11.7
< running_reward >
-3.51
< done >
False
######################################### STEP 62 #########################################
< action >
[  8.61585140e-05  -1.21814609e-03]
< returned_observation >
[ 0.14517683  0.09159706]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 63 #########################################
< action >
[ 0.00088787 -0.00116951]
< returned_observation >
[ 0.1460647   0.09042755]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 64 #########################################
< action >
[ 0.00069979 -0.00126926]
< returned_observation >
[ 0.14676449  0.08915829]
< reward >
-11.600000000000001
< running_reward >
-3.4800000000000004
< done >
False
######################################### STEP 65 #########################################
< action >
[ 0.00156749 -0.00051669]
< returned_observation >
[ 0.14833198  0.08864159]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 66 #########################################
< action >
[ 0.00209429 -0.00037811]
< returned_observation >
[ 0.15042627  0.08826348]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 67 #########################################
< action >
[ 0.00216855  0.00024091]
< returned_observation >
[ 0.15259482  0.08850439]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 68 #########################################
< action >
[ 0.00239736  0.00092305]
< returned_observation >
[ 0.15499218  0.08942744]
< reward >
2.3999999999999986
< running_reward >
0.7199999999999995
< done >
False
######################################### STEP 69 #########################################
< action >
[ 0.00208316 -0.00151336]
< returned_observation >
[ 0.15707534  0.08791408]
< reward >
-11.7
< running_reward >
-3.51
< done >
False
######################################### STEP 70 #########################################
< action >
[ 0.00311845  0.0006762 ]
< returned_observation >
[ 0.16019379  0.08859028]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 71 #########################################
< action >
[ 0.00355451  0.00057489]
< returned_observation >
[ 0.1637483   0.08916517]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 72 #########################################
< action >
[ 0.00341203  0.00045486]
< returned_observation >
[ 0.16716033  0.08962003]
< reward >
-10.8
< running_reward >
-3.24
< done >
False
######################################### STEP 73 #########################################
< action >
[ 0.00342363 -0.00066793]
< returned_observation >
[ 0.17058396  0.0889521 ]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 74 #########################################
< action >
[ 0.00290072 -0.00166382]
< returned_observation >
[ 0.17348468  0.08728827]
< reward >
-11.600000000000001
< running_reward >
-3.4800000000000004
< done >
False
######################################### STEP 75 #########################################
< action >
[ 0.00314459 -0.00195764]
< returned_observation >
[ 0.17662926  0.08533064]
< reward >
-10.8
< running_reward >
-3.24
< done >
False
######################################### STEP 76 #########################################
< action >
[ 0.00363445 -0.00212901]
< returned_observation >
[ 0.18026372  0.08320162]
< reward >
-12.2
< running_reward >
-3.6599999999999997
< done >
False
######################################### STEP 77 #########################################
< action >
[ 0.00461461 -0.00191186]
< returned_observation >
[ 0.18487833  0.08128976]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 78 #########################################
< action >
[ 0.00679746 -0.00380497]
< returned_observation >
[ 0.19167579  0.07748479]
< reward >
-11.0
< running_reward >
-3.3
< done >
False
######################################### STEP 79 #########################################
< action >
[ 0.00613899 -0.00289702]
< returned_observation >
[ 0.19781478  0.07458778]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 80 #########################################
< action >
[ 0.00865688 -0.00314072]
< returned_observation >
[ 0.20647167  0.07144705]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 81 #########################################
< action >
[ 0.00868583 -0.0033617 ]
< returned_observation >
[ 0.2151575   0.06808535]
< reward >
-12.100000000000001
< running_reward >
-3.6300000000000003
< done >
False
######################################### STEP 82 #########################################
< action >
[ 0.01000252 -0.00364329]
< returned_observation >
[ 0.22516002  0.06444206]
< reward >
-11.7
< running_reward >
-3.51
< done >
False
######################################### STEP 83 #########################################
< action >
[ 0.01066559 -0.0039569 ]
< returned_observation >
[ 0.23582561  0.06048516]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 84 #########################################
< action >
[ 0.0093749  -0.00363663]
< returned_observation >
[ 0.24520051  0.05684853]
< reward >
-11.8
< running_reward >
-3.54
< done >
False
######################################### STEP 85 #########################################
< action >
[ 0.00986215 -0.00233722]
< returned_observation >
[ 0.25506266  0.05451132]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 86 #########################################
< action >
[ 0.00973493 -0.00295061]
< returned_observation >
[ 0.26479759  0.05156071]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 87 #########################################
< action >
[ 0.01028115 -0.00270485]
< returned_observation >
[ 0.27507874  0.04885586]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 88 #########################################
< action >
[ 0.01054433 -0.00277337]
< returned_observation >
[ 0.28562307  0.04608249]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 89 #########################################
< action >
[ 0.01168675 -0.00239916]
< returned_observation >
[ 0.29730982  0.04368333]
< reward >
-11.3
< running_reward >
-3.39
< done >
True
##################### episode 2 infomation #####################
{'episode_reward': -327.4000000000001, 'ave_reward': -10.913333333333336, 'max_reward': 2.3999999999999986, 'min_reward': -12.2, 'nb_episode_steps': 30, 'nb_steps': 90}
######################################### STEP 90 #########################################
< action >
[ 0.00065905 -0.00162744]
< returned_observation >
[ 0.18691926  0.34393329]
< reward >
2.6000000000000014
< running_reward >
0.7800000000000004
< done >
False
######################################### STEP 91 #########################################
< action >
[ 0.00126025 -0.00120439]
< returned_observation >
[ 0.18817951  0.3427289 ]
< reward >
-11.0
< running_reward >
-3.3
< done >
False
######################################### STEP 92 #########################################
< action >
[ 0.00206906 -0.0001579 ]
< returned_observation >
[ 0.19024857  0.342571  ]
< reward >
3.3999999999999986
< running_reward >
1.0199999999999996
< done >
False
######################################### STEP 93 #########################################
< action >
[ 0.00166466  0.00066637]
< returned_observation >
[ 0.19191323  0.34323737]
< reward >
-12.2
< running_reward >
-3.6599999999999997
< done >
False
######################################### STEP 94 #########################################
< action >
[ 0.00109953  0.00262012]
< returned_observation >
[ 0.19301276  0.34585749]
< reward >
-12.0
< running_reward >
-3.5999999999999996
< done >
False
######################################### STEP 95 #########################################
< action >
[-0.00023429  0.00085497]
< returned_observation >
[ 0.19277846  0.34671247]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 96 #########################################
< action >
[ -1.88461542e-03  -3.70323658e-05]
< returned_observation >
[ 0.19089385  0.34667544]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 97 #########################################
< action >
[-0.00300053  0.00191911]
< returned_observation >
[ 0.18789332  0.34859454]
< reward >
2.8000000000000007
< running_reward >
0.8400000000000002
< done >
False
######################################### STEP 98 #########################################
< action >
[-0.00332193  0.00057317]
< returned_observation >
[ 0.18457139  0.34916771]
< reward >
-11.8
< running_reward >
-3.54
< done >
False
######################################### STEP 99 #########################################
< action >
[ -2.20189691e-03  -1.43259764e-05]
< returned_observation >
[ 0.18236949  0.34915339]
< reward >
3.1000000000000014
< running_reward >
0.9300000000000004
< done >
False
######################################### STEP 100 #########################################
< action >
[-0.00343501  0.00086154]
< returned_observation >
[ 0.17893449  0.35001493]
< reward >
1.6999999999999993
< running_reward >
0.5099999999999998
< done >
False
######################################### STEP 101 #########################################
< action >
[-0.00280581  0.00042511]
< returned_observation >
[ 0.17612868  0.35044004]
< reward >
-12.100000000000001
< running_reward >
-3.6300000000000003
< done >
False
######################################### STEP 102 #########################################
< action >
[-0.0013685  0.0005285]
< returned_observation >
[ 0.17476019  0.35096853]
< reward >
-11.8
< running_reward >
-3.54
< done >
False
######################################### STEP 103 #########################################
< action >
[ 0.00027583  0.00100542]
< returned_observation >
[ 0.17503602  0.35197395]
< reward >
3.0
< running_reward >
0.8999999999999999
< done >
False
######################################### STEP 104 #########################################
< action >
[ 0.00185933  0.00108821]
< returned_observation >
[ 0.17689535  0.35306216]
< reward >
2.8000000000000007
< running_reward >
0.8400000000000002
< done >
False
######################################### STEP 105 #########################################
< action >
[ 0.0006601   0.00351236]
< returned_observation >
[ 0.17755545  0.35657452]
< reward >
2.1999999999999993
< running_reward >
0.6599999999999998
< done >
False
######################################### STEP 106 #########################################
< action >
[ 0.00013776  0.0032833 ]
< returned_observation >
[ 0.17769321  0.35985782]
< reward >
2.6999999999999993
< running_reward >
0.8099999999999997
< done >
False
######################################### STEP 107 #########################################
< action >
[-0.00053882  0.0030511 ]
< returned_observation >
[ 0.17715439  0.36290892]
< reward >
1.5
< running_reward >
0.44999999999999996
< done >
False
######################################### STEP 108 #########################################
< action >
[-0.00027159  0.00322529]
< returned_observation >
[ 0.1768828   0.36613421]
< reward >
-12.5
< running_reward >
-3.75
< done >
False
######################################### STEP 109 #########################################
< action >
[ 0.00041079  0.00499237]
< returned_observation >
[ 0.17729359  0.37112658]
< reward >
-11.600000000000001
< running_reward >
-3.4800000000000004
< done >
False
######################################### STEP 110 #########################################
< action >
[ 0.00031022  0.00373158]
< returned_observation >
[ 0.17760381  0.37485816]
< reward >
-12.100000000000001
< running_reward >
-3.6300000000000003
< done >
False
######################################### STEP 111 #########################################
< action >
[-0.00085298  0.0030412 ]
< returned_observation >
[ 0.17675083  0.37789936]
< reward >
-11.2
< running_reward >
-3.36
< done >
False
######################################### STEP 112 #########################################
< action >
[-0.00250717  0.00384582]
< returned_observation >
[ 0.17424367  0.38174518]
< reward >
-11.0
< running_reward >
-3.3
< done >
False
######################################### STEP 113 #########################################
< action >
[-0.00298629  0.00351817]
< returned_observation >
[ 0.17125737  0.38526335]
< reward >
-12.0
< running_reward >
-3.5999999999999996
< done >
False
######################################### STEP 114 #########################################
< action >
[-0.00297047  0.00210796]
< returned_observation >
[ 0.16828691  0.38737131]
< reward >
2.8999999999999986
< running_reward >
0.8699999999999996
< done >
False
######################################### STEP 115 #########################################
< action >
[-0.00381326  0.00277101]
< returned_observation >
[ 0.16447365  0.39014232]
< reward >
2.8000000000000007
< running_reward >
0.8400000000000002
< done >
False
######################################### STEP 116 #########################################
< action >
[-0.00317888  0.00232053]
< returned_observation >
[ 0.16129477  0.39246286]
< reward >
-11.600000000000001
< running_reward >
-3.4800000000000004
< done >
False
######################################### STEP 117 #########################################
< action >
[-0.00135905  0.00101386]
< returned_observation >
[ 0.15993572  0.39347672]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 118 #########################################
< action >
[-0.00170389  0.00078977]
< returned_observation >
[ 0.15823184  0.39426649]
< reward >
-11.899999999999999
< running_reward >
-3.5699999999999994
< done >
False
######################################### STEP 119 #########################################
< action >
[-0.00449789  0.00273734]
< returned_observation >
[ 0.15373395  0.39700383]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
True
##################### episode 3 infomation #####################
{'episode_reward': -178.2, 'ave_reward': -5.9399999999999995, 'max_reward': 3.3999999999999986, 'min_reward': -12.5, 'nb_episode_steps': 30, 'nb_steps': 120}
######################################### STEP 120 #########################################
< action >
[ 0.00233865  0.00036948]
< returned_observation >
[ 0.39910613  0.53918621]
< reward >
-11.100000000000001
< running_reward >
-3.3300000000000005
< done >
False
######################################### STEP 121 #########################################
< action >
[ 0.00275905 -0.00022289]
< returned_observation >
[ 0.40186518  0.53896332]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 122 #########################################
< action >
[ 0.00285286 -0.00173128]
< returned_observation >
[ 0.40471804  0.53723204]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 123 #########################################
< action >
[ 0.00363548 -0.00208526]
< returned_observation >
[ 0.40835351  0.53514678]
< reward >
-11.8
< running_reward >
-3.54
< done >
False
######################################### STEP 124 #########################################
< action >
[ 0.00375527 -0.00086136]
< returned_observation >
[ 0.41210879  0.53428542]
< reward >
-11.3
< running_reward >
-3.39
< done >
False
######################################### STEP 125 #########################################
< action >
[ 0.00402365 -0.00057824]
< returned_observation >
[ 0.41613244  0.53370719]
< reward >
3.1999999999999993
< running_reward >
0.9599999999999997
< done >
False
######################################### STEP 126 #########################################
< action >
[ 0.00428271 -0.00129141]
< returned_observation >
[ 0.42041515  0.53241578]
< reward >
3.5
< running_reward >
1.05
< done >
False
######################################### STEP 127 #########################################
< action >
[ 0.00509695  0.00027572]
< returned_observation >
[ 0.42551211  0.5326915 ]
< reward >
-13.0
< running_reward >
-3.9
< done >
False
######################################### STEP 128 #########################################
< action >
[ 0.00583104  0.00118243]
< returned_observation >
[ 0.43134315  0.53387393]
< reward >
0.3999999999999986
< running_reward >
0.11999999999999957
< done >
False
######################################### STEP 129 #########################################
< action >
[ 0.00492497  0.00033634]
< returned_observation >
[ 0.43626812  0.53421027]
< reward >
2.1999999999999993
< running_reward >
0.6599999999999998
< done >
False
######################################### STEP 130 #########################################
< action >
[ 0.0034541   0.00159335]
< returned_observation >
[ 0.43972222  0.53580362]
< reward >
-12.0
< running_reward >
-3.5999999999999996
< done >
False
######################################### STEP 131 #########################################
< action >
[ 0.00317027  0.00301726]
< returned_observation >
[ 0.44289249  0.53882088]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 132 #########################################
< action >
[ 0.00235757  0.00260221]
< returned_observation >
[ 0.44525006  0.54142309]
< reward >
-11.399999999999999
< running_reward >
-3.4199999999999995
< done >
False
######################################### STEP 133 #########################################
< action >
[ 0.0024225   0.00354748]
< returned_observation >
[ 0.44767256  0.54497056]
< reward >
3.5
< running_reward >
1.05
< done >
False
######################################### STEP 134 #########################################
< action >
[ 0.00245166  0.00378146]
< returned_observation >
[ 0.45012422  0.54875202]
< reward >
2.0
< running_reward >
0.6
< done >
False
######################################### STEP 135 #########################################
< action >
[ 0.00243545  0.00398178]
< returned_observation >
[ 0.45255967  0.5527338 ]
< reward >
1.3999999999999986
< running_reward >
0.41999999999999954
< done >
False
######################################### STEP 136 #########################################
< action >
[ 0.00128511  0.00408637]
< returned_observation >
[ 0.45384478  0.55682016]
< reward >
-13.399999999999999
< running_reward >
-4.02
< done >
False
######################################### STEP 137 #########################################
< action >
[ 0.00106611  0.00405325]
< returned_observation >
[ 0.45491089  0.56087341]
< reward >
-11.7
< running_reward >
-3.51
< done >
False
######################################### STEP 138 #########################################
< action >
[ 0.00152586  0.0033967 ]
< returned_observation >
[ 0.45643674  0.56427011]
< reward >
-13.3
< running_reward >
-3.99
< done >
False
######################################### STEP 139 #########################################
< action >
[ 0.00140731  0.00312517]
< returned_observation >
[ 0.45784405  0.56739529]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 140 #########################################
< action >
[ 0.00179034  0.00393683]
< returned_observation >
[ 0.45963439  0.57133211]
< reward >
2.6000000000000014
< running_reward >
0.7800000000000004
< done >
False
######################################### STEP 141 #########################################
< action >
[ 0.00079946  0.00454885]
< returned_observation >
[ 0.46043385  0.57588097]
< reward >
2.8999999999999986
< running_reward >
0.8699999999999996
< done >
False
######################################### STEP 142 #########################################
< action >
[ 0.00283139  0.00310295]
< returned_observation >
[ 0.46326524  0.57898392]
< reward >
1.8999999999999986
< running_reward >
0.5699999999999995
< done >
False
######################################### STEP 143 #########################################
< action >
[ 0.00196375  0.00225031]
< returned_observation >
[ 0.46522899  0.58123423]
< reward >
-11.5
< running_reward >
-3.4499999999999997
< done >
False
######################################### STEP 144 #########################################
< action >
[ 0.00164829  0.00390319]
< returned_observation >
[ 0.46687728  0.58513742]
< reward >
-13.100000000000001
< running_reward >
-3.93
< done >
False
######################################### STEP 145 #########################################
