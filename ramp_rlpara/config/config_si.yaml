replay_buffer_size: 100000 # size(R)
discount_factor: 0.9 # gamma
target_net_update_factor: 0.001 # tao
mini_batch_size: 256 # N
max_exe_time: 60.0 # seconds
time_stamp_max: 25.0 # maximal time stamp in motion state
max_nb_exe: 600 # max number of execution
ornstein_uhlenbeck_paras: {'theta': 10.0, 'percent': 0.33333} # theta and percent of max_nb_exe to make random disappear,
                                                              # but bias need some time to be clear
nb_steps_warmup_critic: 0 # number of steps needed before updating critic
                          # warmup is needed to fill the replay buffer with mini_batch_size data,
                          # so we can sample without replacement.
                          # but in our application, after one execution, many transitions will be returned.
nb_steps_warmup_actor: 0 # number of steps needed before updating actor
critic_lr: 0.001 # learning rate of critic
coe_A_range: [0.0, 0.05] # coefficients of orientation changing of feasible trajectory
coe_D_range: [0.0, 50.0] # coefficients of obstacle distance of feasible trajectory
coe_Qk_range: [0.0, 50.0] # coefficietns of orientation changing of infeasible trajectory